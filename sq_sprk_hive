sudo -u hdfs sqoop import-all-tables --connect "jdbc:mysql://localhost:3306/retail_db" \
--username root --password cloudera \
--num-mappers 3 \
--hive-import \
--hive-database spark_poc \
--create-hive-table 

import org.apache.spark.sql.hive.HiveContext
val hc=new HiveContext(sc)

val dept_result=hc.sql("select * from ( select department_id as dept,category_id,product_price,product_name, rank() over(partition by department_id order by product_price) as Rnk from spark_poc.products p inner join spark_poc.categories c on p.product_category_id=c.category_id inner join spark_poc.departments d on d.department_id=c.category_department_id ) a order by dept asc,Rnk desc")

dept_result.registerTempTable("top_products")

hc.sql("create table spark_poc.top_products row format delimited fields terminated by '|' stored as  textfile as select * from top_products where product_price <100")



val topNcustomers=hc.sql("select customer_id,count(distinct product_id) as uniq_products from spark_poc.products p  inner join spark_poc.order_items oi on oi.order_item_product_id=p.product_id inner join spark_poc.orders o on oi.order_item_order_id=o.order_id inner join spark_poc.customers c on c.customer_id=o.order_customer_id group by customer_id order by uniq_products desc,customer_id asc limit 10")

topNcustomers.registerTempTable("top_customers")

hc.sql("create table spark_poc.top_cust_products row format delimited fields terminated by '#' stored as textfile as select customer_id,product_id,product_name,product_price from spark_poc.products p  inner join spark_poc.order_items oi on oi.order_item_product_id=p.product_id inner join spark_poc.orders o on oi.order_item_order_id=o.order_id inner join top_customers c on c.customer_id=o.order_customer_id  where product_price <100")







